{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0",
   "metadata": {},
   "source": [
    "# Monosemantic Feature Extraction: Two Key Experiments\n",
    "\n",
    "This notebook walks through two experiments that shaped the monosemantic feature extraction algorithm:\n",
    "\n",
    "1. **Switching from cosine similarity to dot product** for the neighbor graph (commit `6eb536c`)\n",
    "2. **Post-hoc spectral gap filtering** as a replacement for the minimality filter in the overcomplete case\n",
    "\n",
    "### Background\n",
    "\n",
    "The algorithm extracts monosemantic features from neural network representations using synthetic data with known ground truth. The pipeline:\n",
    "\n",
    "1. Generate a feature basis (orthogonal or ε-orthogonal) and sparse linear combinations as representations\n",
    "2. Build a neighbor graph — threshold pairwise similarity to identify which reps share a feature\n",
    "3. Find monosemantic targets — reps dominated by a single feature\n",
    "4. For each target: compute the nullspace of non-neighbors, project neighbors onto it, SVD → dominant direction is the extracted feature\n",
    "\n",
    "Both experiments address failures that arise when features are **overcomplete** ($n > d$), forcing $\\varepsilon > 0$ inter-feature correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.config import SyntheticConfig, ExtractionConfig, compute_coef_min\n",
    "from src.synthetic import generate_feature_basis, generate_representations\n",
    "from src.extraction import (\n",
    "    resolve_tau, compute_tau_bounds, compute_nullspace, extract_feature,\n",
    "    extract_all_features, build_neighbor_matrix, find_monosemantic_targets\n",
    ")\n",
    "from src.metrics import evaluate, match_features\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "print(f\"PyTorch {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0",
   "metadata": {},
   "source": [
    "---\n",
    "# Experiment 1: Dot Product vs Cosine Similarity\n",
    "\n",
    "## The Problem\n",
    "\n",
    "The neighbor graph decides which representations \"share a feature.\" Originally, we used **cosine similarity** with a threshold $\\tau$:\n",
    "\n",
    "$$\\text{neighbors}(r_i, r_j) = \\mathbb{1}\\left[\\frac{|r_i \\cdot r_j|}{\\|r_i\\| \\|r_j\\|} \\geq \\tau\\right]$$\n",
    "\n",
    "This works fine when features are orthogonal ($\\varepsilon = 0$). But when $\\varepsilon > 0$, cosine similarity **normalizes away magnitude information** that's critical for distinguishing shared-feature signal from ε-interference noise.\n",
    "\n",
    "## Why Magnitude Matters\n",
    "\n",
    "Consider two representations that share feature $f_1$ with small coefficients $c_{\\min}$:\n",
    "\n",
    "$$r_a = c_{\\min} f_1 + c_{\\max} f_2 + c_{\\max} f_3, \\quad r_b = c_{\\min} f_1 + c_{\\max} f_4 + c_{\\max} f_5$$\n",
    "\n",
    "**Dot product:** $r_a \\cdot r_b = c_{\\min}^2 \\underbrace{\\langle f_1, f_1 \\rangle}_{=1} + \\sum_{\\text{cross}} c_i c_j \\underbrace{\\langle f_i, f_j \\rangle}_{\\leq \\varepsilon}$\n",
    "\n",
    "The shared feature contributes $c_{\\min}^2$. Cross-feature interference contributes at most $k^2 c_{\\max}^2 \\varepsilon$.\n",
    "\n",
    "**Cosine similarity** divides by $\\|r_a\\| \\|r_b\\| \\approx k \\cdot c_{\\max}^2$, shrinking both signal and noise by the same factor. But the noise floor *relative to the signal* stays the same — we've gained nothing from the normalization, and we've lost the ability to set an absolute threshold.\n",
    "\n",
    "**Dot product** preserves the absolute scale: shared signal $\\sim c_{\\min}^2$, noise $\\sim k^2 c_{\\max}^2 \\varepsilon$. These are on completely different scales when $\\varepsilon$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1",
   "metadata": {},
   "source": [
    "## Setup: Overcomplete Features ($n > d$, $\\varepsilon > 0$)\n",
    "\n",
    "We need $\\varepsilon > 0$ to see the difference, so we use an overcomplete basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "d, n_feat = 16, 20  # 20 features in 16 dimensions → overcomplete\n",
    "basis = generate_feature_basis(d, n_feat)\n",
    "eps = basis.achieved_epsilon\n",
    "\n",
    "print(f\"d={d}, n={n_feat}\")\n",
    "print(f\"Achieved ε = {eps:.4f}\")\n",
    "print(f\"Welch bound = {basis.welch_bound:.4f}\")\n",
    "print(f\"Converged: {basis.converged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate representations with controlled coefficients\n",
    "# coef_factor=0 → coef_min = floor = 0.5, coef_max = 1.0\n",
    "# This gives us predictable bounds.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "syn_config = SyntheticConfig(\n",
    "    d=d, n=n_feat, k=2,\n",
    "    num_representations=200,\n",
    "    sparsity_mode='fixed',\n",
    "    positive_only=True,\n",
    "    coef_min_floor=0.5, coef_max=1.0, coef_factor=0.0,\n",
    ")\n",
    "\n",
    "reps, coeffs = generate_representations(basis.features, syn_config, epsilon=eps)\n",
    "\n",
    "coef_min = compute_coef_min(syn_config, eps)\n",
    "print(f\"k={syn_config.k}, coef_min={coef_min:.2f}, coef_max={syn_config.coef_max:.2f}\")\n",
    "print(f\"Representations: {reps.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4",
   "metadata": {},
   "source": [
    "## Computing Ground-Truth Sharing\n",
    "\n",
    "Two representations share a feature iff their support sets overlap. Let's compute the actual dot products and cosine similarities for sharing vs non-sharing pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth: which pairs share a feature?\n",
    "support = (coeffs != 0).float()  # (N, n)\n",
    "gt_sharing = (support @ support.T) > 0  # True if any feature in common\n",
    "\n",
    "# Compute pairwise dot products and cosine similarities\n",
    "dots = reps @ reps.T\n",
    "norms = torch.norm(reps, dim=1, keepdim=True)\n",
    "cossim = dots / (norms @ norms.T + 1e-8)\n",
    "\n",
    "# Extract off-diagonal pairs\n",
    "N = reps.shape[0]\n",
    "mask = torch.triu(torch.ones(N, N, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "sharing = gt_sharing[mask].numpy()\n",
    "dots_flat = torch.abs(dots[mask]).numpy()\n",
    "cossim_flat = torch.abs(cossim[mask]).numpy()\n",
    "\n",
    "print(f\"Pairs: {sharing.sum()} sharing, {(~sharing.astype(bool)).sum()} non-sharing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison: dot product vs cosine similarity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Dot product\n",
    "ax = axes[0]\n",
    "ax.hist(dots_flat[sharing == True], bins=50, alpha=0.6, label='Sharing', color='steelblue', density=True)\n",
    "ax.hist(dots_flat[sharing == False], bins=50, alpha=0.6, label='Non-sharing', color='salmon', density=True)\n",
    "ax.set_xlabel('|dot product|')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Dot product: sharing vs non-sharing')\n",
    "ax.legend()\n",
    "\n",
    "# Cosine similarity\n",
    "ax = axes[1]\n",
    "ax.hist(cossim_flat[sharing == True], bins=50, alpha=0.6, label='Sharing', color='steelblue', density=True)\n",
    "ax.hist(cossim_flat[sharing == False], bins=50, alpha=0.6, label='Non-sharing', color='salmon', density=True)\n",
    "ax.set_xlabel('|cosine similarity|')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Cosine similarity: sharing vs non-sharing')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Separation gaps\n",
    "dot_gap = dots_flat[sharing == True].min() - dots_flat[sharing == False].max()\n",
    "cos_gap = cossim_flat[sharing == True].min() - cossim_flat[sharing == False].max()\n",
    "print(f\"Dot product separation gap:  {dot_gap:+.4f} {'✓ SEPARABLE' if dot_gap > 0 else '✗ overlap'}\")\n",
    "print(f\"Cosine sim separation gap:   {cos_gap:+.4f} {'✓ SEPARABLE' if cos_gap > 0 else '✗ overlap'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7",
   "metadata": {},
   "source": [
    "## The Tau Bounds: Old vs New\n",
    "\n",
    "The old cosine-similarity bounds divided by the maximum representation norm squared, collapsing the gap. The new dot-product bounds operate on the raw quantities:\n",
    "\n",
    "| | Old (cosine sim) | New (dot product) |\n",
    "|---|---|---|\n",
    "| $\\tau_{\\text{upper}}$ (min sharing) | $c_{\\min}^2 / (k \\cdot c_{\\max}^2)$ | $c_{\\min}^2$ |\n",
    "| $\\tau_{\\text{lower}}$ (max non-sharing) | $k \\cdot \\varepsilon$ | $k^2 \\cdot c_{\\max}^2 \\cdot \\varepsilon$ |\n",
    "| Separable when | $c_{\\min}^2 > k^2 c_{\\max}^2 \\varepsilon$ | same condition |\n",
    "\n",
    "The separability *condition* is mathematically equivalent, but the cosine-sim bounds were **empirically wrong** — the normalization interacted poorly with the actual similarity distributions, making the predicted bounds unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and compare bounds\n",
    "k = syn_config.k\n",
    "c_min = coef_min\n",
    "c_max = syn_config.coef_max\n",
    "\n",
    "# New dot product bounds (current code)\n",
    "tau_upper_dot, tau_lower_dot = compute_tau_bounds(k, eps, c_min, c_max)\n",
    "\n",
    "# Old cosine similarity bounds (before commit 6eb536c)\n",
    "max_norm_sq = k * c_max**2\n",
    "tau_upper_cos = c_min * c_min / max_norm_sq\n",
    "tau_lower_cos = k * k * c_max * c_max * eps / max_norm_sq\n",
    "\n",
    "print(\"DOT PRODUCT BOUNDS (current):\")\n",
    "print(f\"  τ_upper = c_min² = {c_min}² = {tau_upper_dot:.4f}\")\n",
    "print(f\"  τ_lower = k²·c_max²·ε = {k}²·{c_max}²·{eps:.4f} = {tau_lower_dot:.4f}\")\n",
    "print(f\"  Gap: {tau_upper_dot - tau_lower_dot:+.4f}\")\n",
    "print(f\"  Separable: {tau_lower_dot < tau_upper_dot}\")\n",
    "\n",
    "print(f\"\\nCOSINE SIMILARITY BOUNDS (old, before 6eb536c):\")\n",
    "print(f\"  τ_upper = c_min²/(k·c_max²) = {c_min}²/({k}·{c_max}²) = {tau_upper_cos:.4f}\")\n",
    "print(f\"  τ_lower = k·ε = {k}·{eps:.4f} = {tau_lower_cos:.4f}\")\n",
    "print(f\"  Gap: {tau_upper_cos - tau_lower_cos:+.4f}\")\n",
    "print(f\"  Separable: {tau_lower_cos < tau_upper_cos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate bounds against actual data\n",
    "actual_min_sharing_dot = dots_flat[sharing == True].min()\n",
    "actual_max_nonsharing_dot = dots_flat[sharing == False].max()\n",
    "actual_min_sharing_cos = cossim_flat[sharing == True].min()\n",
    "actual_max_nonsharing_cos = cossim_flat[sharing == False].max()\n",
    "\n",
    "print(\"BOUNDS vs ACTUAL (dot product):\")\n",
    "print(f\"  τ_upper = {tau_upper_dot:.4f}  ≤  actual min sharing = {actual_min_sharing_dot:.4f}?  {tau_upper_dot <= actual_min_sharing_dot + 1e-4}\")\n",
    "print(f\"  τ_lower = {tau_lower_dot:.4f}  ≥  actual max non-sharing = {actual_max_nonsharing_dot:.4f}?  {tau_lower_dot >= actual_max_nonsharing_dot - 1e-4}\")\n",
    "\n",
    "print(f\"\\nBOUNDS vs ACTUAL (cosine similarity):\")\n",
    "print(f\"  τ_upper = {tau_upper_cos:.4f}  ≤  actual min sharing = {actual_min_sharing_cos:.4f}?  {tau_upper_cos <= actual_min_sharing_cos + 1e-4}\")\n",
    "print(f\"  τ_lower = {tau_lower_cos:.4f}  ≥  actual max non-sharing = {actual_max_nonsharing_cos:.4f}?  {tau_lower_cos >= actual_max_nonsharing_cos - 1e-4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10",
   "metadata": {},
   "source": [
    "## Concrete Example: A Small-Coefficient Monosemantic Rep\n",
    "\n",
    "This is the scenario where cosine similarity breaks. A monosemantic representation with a small coefficient gets normalized to a unit vector, making the shared-feature signal indistinguishable from ε-noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a concrete example by hand\n",
    "torch.manual_seed(42)\n",
    "features = basis.features  # (20, 16)\n",
    "\n",
    "# Monosemantic rep: only feature 0, small coefficient\n",
    "r_mono = 0.5 * features[0]  # small coef on one feature\n",
    "\n",
    "# Sharing rep: features 0 and 3, with large coef on feature 3\n",
    "r_share = 0.5 * features[0] + 1.0 * features[3]\n",
    "\n",
    "# Non-sharing rep: features 5 and 7, large coefficients\n",
    "r_nonshare = 1.0 * features[5] + 1.0 * features[7]\n",
    "\n",
    "# Compute metrics\n",
    "dot_share = torch.abs(r_mono @ r_share).item()\n",
    "dot_nonshare = torch.abs(r_mono @ r_nonshare).item()\n",
    "\n",
    "cos_share = torch.abs(F.cosine_similarity(r_mono.unsqueeze(0), r_share.unsqueeze(0))).item()\n",
    "cos_nonshare = torch.abs(F.cosine_similarity(r_mono.unsqueeze(0), r_nonshare.unsqueeze(0))).item()\n",
    "\n",
    "print(f\"r_mono = 0.5·f₀                  (‖r‖ = {r_mono.norm():.3f})\")\n",
    "print(f\"r_share = 0.5·f₀ + 1.0·f₃        (‖r‖ = {r_share.norm():.3f})\")\n",
    "print(f\"r_nonshare = 1.0·f₅ + 1.0·f₇     (‖r‖ = {r_nonshare.norm():.3f})\")\n",
    "print(f\"\\nε = {eps:.4f} (max inter-feature correlation)\")\n",
    "print(f\"\")\n",
    "print(f\"                        Dot product    Cosine sim\")\n",
    "print(f\"  r_mono · r_share:     {dot_share:>10.4f}     {cos_share:>10.4f}     (SHOULD be high — they share f₀)\")\n",
    "print(f\"  r_mono · r_nonshare:  {dot_nonshare:>10.4f}     {cos_nonshare:>10.4f}     (SHOULD be low — no shared feature)\")\n",
    "\n",
    "print(f\"\\nDot product gap:  {dot_share - dot_nonshare:+.4f}\")\n",
    "print(f\"Cosine sim gap:   {cos_share - cos_nonshare:+.4f}\")\n",
    "\n",
    "if dot_share > dot_nonshare and cos_share < cos_nonshare:\n",
    "    print(f\"\\n⚠️  Cosine similarity REVERSED the order! Non-sharing looks more similar.\")\n",
    "    print(f\"    Dot product gets it right.\")\n",
    "elif dot_share - dot_nonshare > cos_share - cos_nonshare:\n",
    "    print(f\"\\n→ Dot product provides cleaner separation ({dot_share - dot_nonshare:.4f} vs {cos_share - cos_nonshare:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break down the dot product to show WHY\n",
    "print(\"Decomposing r_mono · r_share:\")\n",
    "shared_signal = 0.5 * 0.5 * (features[0] @ features[0]).item()\n",
    "cross_03 = 0.5 * 1.0 * (features[0] @ features[3]).item()\n",
    "print(f\"  Shared feature (f₀·f₀): 0.5 × 0.5 × ⟨f₀,f₀⟩ = {shared_signal:.4f}\")\n",
    "print(f\"  Cross-term (f₀·f₃):     0.5 × 1.0 × ⟨f₀,f₃⟩ = {cross_03:.4f}  (ε-interference)\")\n",
    "print(f\"  Total: {shared_signal + cross_03:.4f}\")\n",
    "\n",
    "print(f\"\\nDecomposing r_mono · r_nonshare:\")\n",
    "cross_05 = 0.5 * 1.0 * (features[0] @ features[5]).item()\n",
    "cross_07 = 0.5 * 1.0 * (features[0] @ features[7]).item()\n",
    "print(f\"  Cross-term (f₀·f₅):     0.5 × 1.0 × ⟨f₀,f₅⟩ = {cross_05:.4f}  (ε-interference)\")\n",
    "print(f\"  Cross-term (f₀·f₇):     0.5 × 1.0 × ⟨f₀,f₇⟩ = {cross_07:.4f}  (ε-interference)\")\n",
    "print(f\"  Total: {cross_05 + cross_07:.4f}  (pure noise)\")\n",
    "\n",
    "print(f\"\\n→ With dot product, the shared signal ({shared_signal:.4f}) dominates.\")\n",
    "print(f\"  Cosine similarity divides by ‖r_mono‖·‖r_share‖ = {r_mono.norm():.3f}·{r_share.norm():.3f} = {(r_mono.norm() * r_share.norm()):.3f}\")\n",
    "print(f\"  This normalization amplifies the relative contribution of ε-noise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13",
   "metadata": {},
   "source": [
    "## End-to-End Validation\n",
    "\n",
    "Run the full extraction pipeline with dot product (current code) and compare to what cosine similarity would give."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product: use the current code directly\n",
    "torch.manual_seed(42)\n",
    "tau_dot = resolve_tau(ExtractionConfig(tau=None, tau_margin=0.5, epsilon=0.0), syn_config, epsilon=eps)\n",
    "\n",
    "nbr_mat_dot = build_neighbor_matrix(reps, tau_dot)\n",
    "targets_dot = find_monosemantic_targets(nbr_mat_dot)\n",
    "\n",
    "extracted_dot = extract_all_features(\n",
    "    reps, ExtractionConfig(tau=None, tau_margin=0.5, epsilon=0.0),\n",
    "    syn_config, use_minimality_filter=True, basis_epsilon=eps\n",
    ")\n",
    "\n",
    "metrics_dot = evaluate(extracted_dot, basis.features, reps, coeffs, match_threshold=0.9)\n",
    "\n",
    "print(f\"DOT PRODUCT (current, τ={tau_dot:.4f}):\")\n",
    "print(f\"  Targets found: {len(targets_dot)}\")\n",
    "print(f\"  Features extracted: {extracted_dot.shape[0]}\")\n",
    "print(f\"  Recovery rate: {metrics_dot.recovery_rate:.1%} ({int(metrics_dot.recovery_rate * n_feat)}/{n_feat})\")\n",
    "print(f\"  Mean alignment: {metrics_dot.mean_alignment:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity: simulate the OLD behavior\n",
    "# The old code normalized rows then thresholded cosine sim\n",
    "def build_neighbor_matrix_cosine(representations, tau):\n",
    "    \"\"\"OLD behavior: cosine similarity neighbor graph.\"\"\"\n",
    "    norms = torch.norm(representations, dim=1, keepdim=True)\n",
    "    X_norm = representations / (norms + 1e-8)\n",
    "    cossim = X_norm @ X_norm.T\n",
    "    return torch.abs(cossim) >= tau\n",
    "\n",
    "# Use the old tau bounds\n",
    "tau_cos = tau_lower_cos + 0.5 * (tau_upper_cos - tau_lower_cos)\n",
    "\n",
    "nbr_mat_cos = build_neighbor_matrix_cosine(reps, tau_cos)\n",
    "\n",
    "# Check neighbor graph quality\n",
    "off_diag = ~torch.eye(N, dtype=torch.bool)\n",
    "\n",
    "for label, nbr_mat, tau_val in [('Dot product', nbr_mat_dot, tau_dot), ('Cosine sim (old)', nbr_mat_cos, tau_cos)]:\n",
    "    pred = nbr_mat[off_diag]\n",
    "    actual = gt_sharing[off_diag]\n",
    "    tp = (pred & actual).sum().item()\n",
    "    fp = (pred & ~actual).sum().item()\n",
    "    fn = (~pred & actual).sum().item()\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    print(f\"{label} (τ={tau_val:.4f}):  precision={prec:.3f}  recall={rec:.3f}  TP={tp}  FP={fp}  FN={fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "\n",
    "Dot product preserves magnitude, giving clean separation between sharing and non-sharing pairs. Cosine similarity normalizes this away, causing overlap in the distributions and misclassification in the neighbor graph. The switch to dot product (commit `6eb536c`) was essential for the overcomplete case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0",
   "metadata": {},
   "source": [
    "---\n",
    "# Experiment 2: Post-Hoc Spectral Gap Filtering\n",
    "\n",
    "## The Problem: Minimality Filter Fails at 0% Recall\n",
    "\n",
    "The standard pipeline uses a **minimality filter**: select representations whose neighbor count is a local minimum (fewer neighbors than all their neighbors). The intuition is that monosemantic reps, dominated by one feature, should have small neighbor sets.\n",
    "\n",
    "In the overcomplete case with Bernoulli-Gaussian sparsity, this intuition is **exactly backwards**:\n",
    "\n",
    "- **Monosemantic reps** (k=1) align with a single feature that may appear in *many* other representations → **high** neighbor count\n",
    "- **Multi-feature reps** with unusual feature combinations may share features with *few* others → **low** neighbor count\n",
    "\n",
    "The minimality filter picks low-neighbor-count reps as targets — these are the multi-feature ones. **0% recall on truly monosemantic representations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the failure: overcomplete + Bernoulli-Gaussian\n",
    "torch.manual_seed(42)\n",
    "\n",
    "syn_config_oc = SyntheticConfig(\n",
    "    d=50, n=60,\n",
    "    num_representations=500,\n",
    "    sparsity_mode='bernoulli_gaussian',\n",
    "    k=6,  # expected active features\n",
    "    positive_only=True,\n",
    "    coef_factor=10.0, coef_max=1.0, coef_min_floor=0.1,\n",
    ")\n",
    "\n",
    "basis_oc = generate_feature_basis(d=50, n=60)\n",
    "eps_oc = basis_oc.achieved_epsilon\n",
    "reps_oc, coeffs_oc = generate_representations(basis_oc.features, syn_config_oc, epsilon=eps_oc)\n",
    "\n",
    "# Find truly monosemantic reps\n",
    "nnz_oc = (coeffs_oc != 0).sum(dim=1)\n",
    "true_mono_indices = set(torch.where(nnz_oc == 1)[0].tolist())\n",
    "\n",
    "print(f\"d={syn_config_oc.d}, n={syn_config_oc.n}, ε={eps_oc:.4f}\")\n",
    "print(f\"Sparsity distribution (BG, expected k={syn_config_oc.k}):\")\n",
    "for k_val, count in sorted(Counter(nnz_oc.tolist()).items())[:10]:\n",
    "    bar = '█' * (count // 5)\n",
    "    print(f\"  k={k_val:2d}: {count:3d} {bar}\")\n",
    "print(f\"\\nTruly monosemantic (k=1): {len(true_mono_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neighbor graph and run minimality filter\n",
    "# Use a manual tau since auto-tau can be unreliable with BG sparsity\n",
    "tau_oc = resolve_tau(\n",
    "    ExtractionConfig(tau=None, tau_margin=0.5, epsilon=0.0),\n",
    "    syn_config_oc, epsilon=eps_oc\n",
    ")\n",
    "print(f\"Auto-derived τ = {tau_oc:.4f}\")\n",
    "\n",
    "nbr_mat_oc = build_neighbor_matrix(reps_oc, tau_oc)\n",
    "nbr_counts_oc = nbr_mat_oc.sum(dim=1)\n",
    "\n",
    "# Minimality filter\n",
    "targets_oc = find_monosemantic_targets(nbr_mat_oc)\n",
    "target_set = set(targets_oc.tolist())\n",
    "\n",
    "# How many truly monosemantic reps were selected?\n",
    "true_pos = true_mono_indices & target_set\n",
    "recall = len(true_pos) / len(true_mono_indices) if len(true_mono_indices) > 0 else 0\n",
    "\n",
    "print(f\"\\nMinimality filter results:\")\n",
    "print(f\"  Targets selected: {len(targets_oc)}\")\n",
    "print(f\"  Truly monosemantic among targets: {len(true_pos)}\")\n",
    "print(f\"  Recall on k=1 reps: {recall:.0%}\")\n",
    "\n",
    "target_sparsities = nnz_oc[targets_oc]\n",
    "print(f\"  Sparsity of selected targets: {Counter(target_sparsities.tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHY: monosemantic reps have MORE neighbors, not fewer\n",
    "if len(true_mono_indices) > 0:\n",
    "    mono_idx = torch.tensor(list(true_mono_indices))\n",
    "    mono_nbr = nbr_counts_oc[mono_idx].float()\n",
    "    multi_idx = torch.tensor([i for i in range(500) if i not in true_mono_indices])\n",
    "    multi_nbr = nbr_counts_oc[multi_idx].float()\n",
    "\n",
    "    print(f\"Neighbor counts:\")\n",
    "    print(f\"  Monosemantic (k=1):  mean={mono_nbr.mean():.1f}, median={mono_nbr.median():.1f}\")\n",
    "    print(f\"  Multi-feature (k>1): mean={multi_nbr.mean():.1f}, median={multi_nbr.median():.1f}\")\n",
    "    print(f\"\")\n",
    "    if mono_nbr.mean() > multi_nbr.mean():\n",
    "        print(f\"→ Monosemantic reps have MORE neighbors ({mono_nbr.mean():.0f} vs {multi_nbr.mean():.0f})!\")\n",
    "        print(f\"  The minimality filter selects the opposite of what we want.\")\n",
    "    else:\n",
    "        print(f\"→ In this sample, mono reps have fewer neighbors — filter may work.\")\n",
    "else:\n",
    "    print(\"No k=1 reps in this sample (BG sparsity is stochastic).\")\n",
    "    # Show neighbor count distribution by sparsity level\n",
    "    print(f\"\\nNeighbor counts by sparsity:\")\n",
    "    for k_val in sorted(set(nnz_oc.tolist()))[:8]:\n",
    "        k_mask = (nnz_oc == k_val)\n",
    "        if k_mask.sum() > 0:\n",
    "            k_nbr = nbr_counts_oc[k_mask].float()\n",
    "            print(f\"  k={k_val}: mean={k_nbr.mean():.1f} neighbors ({k_mask.sum()} reps)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: neighbor counts by sparsity level\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "if len(true_mono_indices) > 0:\n",
    "    ax.hist(mono_nbr.numpy(), bins=30, alpha=0.6, label='k=1 (monosemantic)', density=True, color='steelblue')\n",
    "    ax.hist(multi_nbr.numpy(), bins=30, alpha=0.5, label='k>1 (multi-feature)', density=True, color='salmon')\n",
    "else:\n",
    "    # Color by low-k vs high-k\n",
    "    low_k = (nnz_oc <= 3)\n",
    "    high_k = (nnz_oc > 6)\n",
    "    if low_k.sum() > 0:\n",
    "        ax.hist(nbr_counts_oc[low_k].float().numpy(), bins=30, alpha=0.6, label='k≤3 (sparse)', density=True, color='steelblue')\n",
    "    if high_k.sum() > 0:\n",
    "        ax.hist(nbr_counts_oc[high_k].float().numpy(), bins=30, alpha=0.5, label='k>6 (dense)', density=True, color='salmon')\n",
    "\n",
    "ax.set_xlabel('Neighbor count')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Neighbor count distribution by true sparsity')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5",
   "metadata": {},
   "source": [
    "## The Alternative: Extract Everything, Filter by Spectral Gap\n",
    "\n",
    "Since we can't identify monosemantic targets *before* extraction, we flip the approach:\n",
    "\n",
    "1. **Extract from ALL unique neighbor-set representatives** (disable minimality filter)\n",
    "2. For each extraction, record the singular value spectrum from the nullspace projection\n",
    "3. **Use $\\sigma_1 / \\sigma_2$ (the spectral gap) to identify monosemantic extractions post-hoc**\n",
    "\n",
    "The intuition: when a target's neighbors truly share one dominant feature, projecting them onto the nullspace of non-neighbors yields a rank-1 matrix — one huge singular value, everything else negligible. A large spectral gap $\\sigma_1 / \\sigma_2 \\gg 1$ is the signature of monosemanticity.\n",
    "\n",
    "Multi-feature targets will have more evenly distributed singular values (multiple significant directions in the projection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from ALL unique neighbor-set representatives, recording spectral gaps\n",
    "from src.extraction import cluster_by_neighbors\n",
    "\n",
    "clusters_oc = cluster_by_neighbors(reps_oc, tau_oc)\n",
    "print(f\"Unique neighbor sets: {len(clusters_oc)}\")\n",
    "\n",
    "spectral_gaps = []\n",
    "all_extracted = []\n",
    "all_target_idx = []\n",
    "all_singular_values = []\n",
    "failed = 0\n",
    "\n",
    "for neighbor_set, member_indices in clusters_oc.items():\n",
    "    target_idx = member_indices[0]\n",
    "    nbr_indices = torch.tensor(sorted(neighbor_set), dtype=torch.long)\n",
    "\n",
    "    if len(nbr_indices) < 2:\n",
    "        failed += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        nullspace = compute_nullspace(reps_oc, nbr_indices, epsilon=0.0)\n",
    "        if nullspace.shape[0] == 0:\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        neighbors = reps_oc[nbr_indices]\n",
    "        projected = neighbors @ nullspace.T @ nullspace\n",
    "        _, S, Vh = torch.linalg.svd(projected, full_matrices=False)\n",
    "\n",
    "        feature = Vh[0]\n",
    "        gap = (S[0] / S[1]).item() if len(S) > 1 and S[1] > 1e-10 else float('inf')\n",
    "\n",
    "        all_extracted.append(feature)\n",
    "        spectral_gaps.append(gap)\n",
    "        all_target_idx.append(target_idx)\n",
    "        all_singular_values.append(S[:5].tolist())\n",
    "    except (ValueError, RuntimeError):\n",
    "        failed += 1\n",
    "        continue\n",
    "\n",
    "all_extracted_t = torch.stack(all_extracted) if all_extracted else torch.empty(0, 50)\n",
    "spectral_gaps_t = torch.tensor(spectral_gaps)\n",
    "\n",
    "print(f\"Successful extractions: {len(all_extracted)}\")\n",
    "print(f\"Failed: {failed}\")\n",
    "print(f\"Spectral gap — min: {spectral_gaps_t.min():.2f}, median: {spectral_gaps_t.median():.2f}, max: {spectral_gaps_t.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match each extraction to ground truth\n",
    "sims_to_gt = torch.abs(all_extracted_t @ basis_oc.features.T)  # (m, n)\n",
    "best_sim_per = sims_to_gt.max(dim=1).values\n",
    "is_correct = best_sim_per > 0.9\n",
    "\n",
    "print(f\"Extractions matching a GT feature (|cos_sim| > 0.9): {is_correct.sum().item()} / {len(all_extracted)}\")\n",
    "\n",
    "correct_gaps = spectral_gaps_t[is_correct]\n",
    "incorrect_gaps = spectral_gaps_t[~is_correct]\n",
    "\n",
    "print(f\"\\nSpectral gaps — correct:   median={correct_gaps.median():.2f}, mean={correct_gaps.mean():.2f}\" if is_correct.sum() > 0 else \"No correct extractions\")\n",
    "print(f\"Spectral gaps — incorrect: median={incorrect_gaps.median():.2f}, mean={incorrect_gaps.mean():.2f}\" if (~is_correct).sum() > 0 else \"All correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral gap histogram: correct vs incorrect\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "max_display = min(spectral_gaps_t.quantile(0.98).item() * 1.5, 100)\n",
    "\n",
    "if is_correct.sum() > 0:\n",
    "    ax.hist(correct_gaps.clamp(max=max_display).numpy(), bins=40, alpha=0.6,\n",
    "            label=f'Correct ({is_correct.sum().item()})', color='steelblue')\n",
    "if (~is_correct).sum() > 0:\n",
    "    ax.hist(incorrect_gaps.clamp(max=max_display).numpy(), bins=40, alpha=0.6,\n",
    "            label=f'Incorrect ({(~is_correct).sum().item()})', color='salmon')\n",
    "ax.set_xlabel('Spectral gap (σ₁/σ₂)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Spectral gap distribution: correct vs incorrect extractions')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9",
   "metadata": {},
   "source": [
    "## Precision/Recall at Different Spectral Gap Thresholds\n",
    "\n",
    "We can sweep the spectral gap threshold to trade off precision (fraction of selected features that are correct) vs recall (fraction of ground-truth features recovered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extraction import _deduplicate_features\n",
    "\n",
    "thresholds = [1.0, 1.5, 2.0, 3.0, 5.0, 8.0, 10.0, 15.0, 20.0, 30.0, 50.0]\n",
    "n_gt = syn_config_oc.n\n",
    "\n",
    "print(f\"{'Threshold':>10} {'Selected':>8} {'Deduped':>7} {'Matched':>7} {'Precision':>9} {'Recall':>7}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "pr_precisions = []\n",
    "pr_recalls = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    sel = spectral_gaps_t >= thresh\n",
    "    if sel.sum() == 0:\n",
    "        print(f\"{thresh:>10.1f} {'—':>8}\")\n",
    "        pr_precisions.append(1.0)\n",
    "        pr_recalls.append(0.0)\n",
    "        continue\n",
    "\n",
    "    sel_features = all_extracted_t[sel]\n",
    "    if sel_features.shape[0] > 1:\n",
    "        sel_features = _deduplicate_features(sel_features, threshold=0.95)\n",
    "\n",
    "    matching, scores = match_features(sel_features, basis_oc.features, threshold=0.9)\n",
    "    prec = len(matching) / sel_features.shape[0] if sel_features.shape[0] > 0 else 0\n",
    "    rec = len(matching) / n_gt\n",
    "\n",
    "    pr_precisions.append(prec)\n",
    "    pr_recalls.append(rec)\n",
    "\n",
    "    print(f\"{thresh:>10.1f} {sel.sum().item():>8} {sel_features.shape[0]:>7} {len(matching):>7} {prec:>8.1%} {rec:>7.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Precision and recall vs threshold\n",
    "ax = axes[0]\n",
    "ax.plot(thresholds, pr_precisions, 'b-o', markersize=5, linewidth=2, label='Precision')\n",
    "ax.plot(thresholds, pr_recalls, 'r-s', markersize=5, linewidth=2, label='Recall')\n",
    "ax.set_xlabel('Spectral gap threshold (σ₁/σ₂)')\n",
    "ax.set_ylabel('Rate')\n",
    "ax.set_title('Precision/Recall vs spectral gap threshold')\n",
    "ax.set_xscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# PR curve\n",
    "ax = axes[1]\n",
    "ax.plot(pr_recalls, pr_precisions, 'g-o', markersize=5, linewidth=2)\n",
    "for i, t in enumerate(thresholds):\n",
    "    if i % 2 == 0 and pr_recalls[i] > 0:\n",
    "        ax.annotate(f'{t:.0f}', (pr_recalls[i], pr_precisions[i]),\n",
    "                    textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision-Recall curve')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12",
   "metadata": {},
   "source": [
    "## Multi-Feature Targets Can Yield Correct Features\n",
    "\n",
    "An interesting finding: you don't *need* a truly monosemantic target (k=1) to extract a correct feature. If one feature in a multi-feature target dominates the neighbor set's shared direction, the SVD still picks it out. The spectral gap captures this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each extraction, check the true sparsity of the target rep\n",
    "target_true_k = nnz_oc[torch.tensor(all_target_idx)]\n",
    "\n",
    "print(f\"{'k':>4} | {'Total':>6} | {'Correct':>7} | {'Rate':>6} | {'Median gap':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for k_val in sorted(set(target_true_k.tolist())):\n",
    "    mask_k = (target_true_k == k_val)\n",
    "    total = mask_k.sum().item()\n",
    "    correct_k = (is_correct & mask_k).sum().item()\n",
    "    gaps_k = spectral_gaps_t[mask_k]\n",
    "    med_gap = gaps_k.median().item() if len(gaps_k) > 0 else 0\n",
    "    print(f\"{k_val:>4} | {total:>6} | {correct_k:>7} | {correct_k/total if total > 0 else 0:>5.1%} | {med_gap:>10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a concrete example: singular value spectrum from a good vs bad extraction\n",
    "# Find the extraction with the highest spectral gap that's correct\n",
    "if is_correct.sum() > 0:\n",
    "    correct_indices = torch.where(is_correct)[0]\n",
    "    best_correct = correct_indices[spectral_gaps_t[correct_indices].argmax()].item()\n",
    "\n",
    "    # Find a bad extraction (low gap, incorrect)\n",
    "    if (~is_correct).sum() > 0:\n",
    "        incorrect_indices = torch.where(~is_correct)[0]\n",
    "        worst_incorrect = incorrect_indices[spectral_gaps_t[incorrect_indices].argmin()].item()\n",
    "    else:\n",
    "        worst_incorrect = None\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Good extraction\n",
    "    sv_good = all_singular_values[best_correct]\n",
    "    ax = axes[0]\n",
    "    ax.bar(range(1, len(sv_good)+1), sv_good, color='steelblue', edgecolor='black')\n",
    "    ax.set_xlabel('Singular value index')\n",
    "    ax.set_ylabel('σ')\n",
    "    k_good = nnz_oc[all_target_idx[best_correct]].item()\n",
    "    ax.set_title(f'Correct extraction (gap={spectral_gaps[best_correct]:.1f}, target k={k_good})')\n",
    "\n",
    "    # Bad extraction\n",
    "    if worst_incorrect is not None:\n",
    "        sv_bad = all_singular_values[worst_incorrect]\n",
    "        ax = axes[1]\n",
    "        ax.bar(range(1, len(sv_bad)+1), sv_bad, color='salmon', edgecolor='black')\n",
    "        ax.set_xlabel('Singular value index')\n",
    "        ax.set_ylabel('σ')\n",
    "        k_bad = nnz_oc[all_target_idx[worst_incorrect]].item()\n",
    "        ax.set_title(f'Incorrect extraction (gap={spectral_gaps[worst_incorrect]:.1f}, target k={k_bad})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Left: one dominant singular value → monosemantic direction found\")\n",
    "    print(f\"Right: multiple significant singular values → no single dominant feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Experiment 1: Cosine Similarity → Dot Product\n",
    "\n",
    "**Problem:** Cosine similarity normalizes by $\\|r_i\\|\\|r_j\\|$, destroying magnitude information. When $\\varepsilon > 0$, small-coefficient shared features become indistinguishable from cross-feature interference after normalization.\n",
    "\n",
    "**Fix:** Switch to raw dot product. The new $\\tau$ bounds are:\n",
    "$$\\tau_{\\text{upper}} = c_{\\min}^2 \\quad \\text{(min sharing signal)}$$\n",
    "$$\\tau_{\\text{lower}} = k^2 c_{\\max}^2 \\varepsilon \\quad \\text{(max non-sharing noise)}$$\n",
    "\n",
    "Separable when $c_{\\min}^2 > k^2 c_{\\max}^2 \\varepsilon$. The bounds directly reflect the physics of the data generation.\n",
    "\n",
    "## Experiment 2: Post-Hoc Spectral Gap Filtering\n",
    "\n",
    "**Problem:** The minimality filter (select targets with fewest neighbors) has 0% recall in the overcomplete case. Monosemantic reps have *high* neighbor counts because their single feature is shared widely. Multi-feature reps with unusual combinations have *low* counts.\n",
    "\n",
    "**Fix:** Extract from *all* unique neighbor-set representatives, then filter post-hoc using the spectral gap $\\sigma_1 / \\sigma_2$ from the nullspace-projected SVD. A large gap indicates a single dominant feature direction was found — the hallmark of a monosemantic extraction.\n",
    "\n",
    "**Key finding:** Even multi-feature targets can yield correct features if one feature dominates the neighbor set's shared direction. The spectral gap captures this automatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}